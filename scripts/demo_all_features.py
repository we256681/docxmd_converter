#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–±–æ—Ç—É –≤—Å–µ—Ö —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from utils.content_analyzers import ContentProcessor, SmartContentExtractor
from docxmd_converter.intelligent_processor import (
    IntelligentProcessor,
    IntelligentQualityAssessor,
    NLPAnalyzer,
)


class FeatureDemo:
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self):
        self.base_dir = Path("/home/uduntu33/–î–æ–∫—É–º–µ–Ω—Ç—ã/PROJECT/docxmd_converter")
        self.conversion_dir = self.base_dir / "docs" / "Conversion"

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
        self.intelligent_processor = IntelligentProcessor()
        self.content_processor = ContentProcessor()
        self.nlp_analyzer = NLPAnalyzer()
        self.quality_assessor = IntelligentQualityAssessor()
        self.content_extractor = SmartContentExtractor()

        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π")
        print("=" * 60)

    def demo_nlp_analysis(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è NLP –∞–Ω–∞–ª–∏–∑–∞"""

        print("\nüß† –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø NLP –ê–ù–ê–õ–ò–ó–ê")
        print("-" * 40)

        # –ë–µ—Ä–µ–º –æ–¥–∏–Ω –∏–∑ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        sample_file = self.conversion_dir / "–ê—Ä—Ö–∏–≤–∏—Å—Ç.md"

        if not sample_file.exists():
            print("‚ùå –§–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        with open(sample_file, "r", encoding="utf-8") as f:
            content = f.read()

        # –û—Ç–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if "<!-- METADATA" in content:
            main_content = content.split("<!-- METADATA")[0]
        else:
            main_content = content

        print(f"üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª: {sample_file.name}")

        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\n1Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
        features = self.nlp_analyzer.extract_features(main_content)

        print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {features.word_count}")
        print(f"   üìù –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {features.sentence_count}")
        print(f"   üìã –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {features.paragraph_count}")
        print(f"   üè∑Ô∏è  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {features.heading_count}")
        print(f"   üìÉ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏—Å–∫–æ–≤: {features.list_count}")
        print(f"   üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {features.avg_sentence_length:.1f}")
        print(f"   üéØ –ë–æ–≥–∞—Ç—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è: {features.vocabulary_richness:.3f}")
        print(f"   üèóÔ∏è  –°–ª–æ–∂–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {features.structure_complexity:.2f}")
        print(f"   üé© –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å: {features.formality_score:.3f}")

        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        print("\n2Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π:")
        entities = self.nlp_analyzer.extract_entities(main_content)

        for entity_type, entity_list in entities.items():
            if entity_list:
                print(
                    f"   {entity_type}: {', '.join(entity_list[:3])}{'...' if len(entity_list) > 3 else ''}"
                )

        # 3. –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        print("\n3Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:")
        sentiment = self.nlp_analyzer.analyze_sentiment(main_content)

        print(f"   üòä –ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è: {sentiment['positive']:.3f}")
        print(f"   üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è: {sentiment['neutral']:.3f}")
        print(f"   üòû –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è: {sentiment['negative']:.3f}")

    def demo_quality_assessment(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""

        print("\nüìä –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –û–¶–ï–ù–ö–ò –ö–ê–ß–ï–°–¢–í–ê")
        print("-" * 40)

        sample_file = self.conversion_dir / "–ê—Ä—Ö–∏–≤–∏—Å—Ç.md"

        if not sample_file.exists():
            print("‚ùå –§–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        with open(sample_file, "r", encoding="utf-8") as f:
            content = f.read()

        if "<!-- METADATA" in content:
            main_content = content.split("<!-- METADATA")[0]
        else:
            main_content = content

        print(f"üìÑ –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ: {sample_file.name}")

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality = self.quality_assessor.assess_quality(
            main_content, "–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–∞—è_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", {}
        )

        print(f"\nüèÜ –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê: {quality.overall_score:.1f}/100")
        print(f"   üèóÔ∏è  –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {quality.structure_score:.1f}/100")
        print(f"   üìù –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {quality.content_score:.1f}/100")
        print(f"   üé® –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {quality.formatting_score:.1f}/100")
        print(f"   ‚úÖ –ü–æ–ª–Ω–æ—Ç–∞: {quality.completeness_score:.1f}/100")
        print(f"   üîÑ –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {quality.consistency_score:.1f}/100")

        if quality.recommendations:
            print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:")
            for i, rec in enumerate(quality.recommendations, 1):
                print(f"   {i}. {rec}")

        if quality.critical_issues:
            print(f"\n‚ö†Ô∏è  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for i, issue in enumerate(quality.critical_issues, 1):
                print(f"   {i}. {issue}")

    def demo_content_analysis(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""

        print("\nüîç –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ê–ù–ê–õ–ò–ó–ê –ö–û–ù–¢–ï–ù–¢–ê")
        print("-" * 40)

        sample_file = self.conversion_dir / "–ê—Ä—Ö–∏–≤–∏—Å—Ç.md"

        if not sample_file.exists():
            print("‚ùå –§–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        with open(sample_file, "r", encoding="utf-8") as f:
            content = f.read()

        if "<!-- METADATA" in content:
            main_content = content.split("<!-- METADATA")[0]
        else:
            main_content = content

        print(f"üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç: {sample_file.name}")

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        cleaned_content, analysis_results = self.content_processor.process_content(
            main_content
        )

        print(f"\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
        for analyzer_name, elements in analysis_results.items():
            if elements:
                print(f"   {analyzer_name.title()}: –Ω–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                for i, element in enumerate(elements[:2]):
                    print(
                        f"     - {element.type} (—É—Ä–æ–≤–µ–Ω—å {element.level}): {element.content[:50]}..."
                    )

        # –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        print(f"\nüß† –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        structured = self.content_extractor.extract_structured_content(main_content)

        for section_name, section_content in list(structured.items())[:3]:
            print(f"   üìë {section_name}: {len(section_content)} —Å–∏–º–≤–æ–ª–æ–≤")

    def demo_document_processing(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

        print("\n‚öôÔ∏è  –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –û–ë–†–ê–ë–û–¢–ö–ò –î–û–ö–£–ú–ï–ù–¢–û–í")
        print("-" * 40)

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        md_files = list(self.conversion_dir.glob("*.md"))

        if not md_files:
            print("‚ùå –§–∞–π–ª—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(md_files)}")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —Ñ–∞–π–ª–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        demo_files = md_files[:3]

        results = []

        for md_file in demo_files:
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {md_file.name}")

            start_time = time.time()

            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            result = self.intelligent_processor.process_document_intelligently(md_file)

            processing_time = time.time() - start_time

            if result["success"]:
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                print(f"   üß† –¢–∏–ø: {result['document_type']}")
                print(f"   üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
                print(
                    f"   üìä –ö–∞—á–µ—Å—Ç–≤–æ: {result['quality_assessment'].overall_score:.1f}/100"
                )
                print(f"   üîß –£–ª—É—á—à–µ–Ω–∏–π: {result['improvements_applied']}")
                print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {processing_time:.3f}—Å")

                results.append(result)
            else:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if results:
            avg_quality = sum(
                r["quality_assessment"].overall_score for r in results
            ) / len(results)
            avg_confidence = sum(r["confidence"] for r in results) / len(results)
            total_improvements = sum(r["improvements_applied"] for r in results)

            print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–ò:")
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)}")
            print(f"   üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {avg_quality:.1f}/100")
            print(f"   üéØ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.2f}")
            print(f"   üîß –í—Å–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏–π: {total_improvements}")

    def demo_template_system(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —à–∞–±–ª–æ–Ω–æ–≤"""

        print("\nüìã –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ –®–ê–ë–õ–û–ù–û–í")
        print("-" * 40)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —à–∞–±–ª–æ–Ω–æ–≤
        config_path = self.base_dir / "document_templates.json"

        if not config_path.exists():
            print("‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        templates = config.get("templates", {})

        print(f"üìö –î–æ—Å—Ç—É–ø–Ω–æ —à–∞–±–ª–æ–Ω–æ–≤: {len(templates)}")

        for template_id, template_data in templates.items():
            print(f"\nüìÑ {template_data['name']} ({template_id})")
            print(f"   üìù –û–ø–∏—Å–∞–Ω–∏–µ: {template_data.get('description', '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è')}")
            print(f"   üìã –†–∞–∑–¥–µ–ª–æ–≤: {len(template_data.get('sections', []))}")
            print(
                f"   ‚úÖ –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö: {len(template_data.get('required_sections', []))}"
            )
            print(
                f"   üîç –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(template_data.get('detection_keywords', []))}"
            )
            print(
                f"   üé® –ù—É–º–µ—Ä–∞—Ü–∏—è: {template_data.get('numbering_pattern', '–Ω–µ —É–∫–∞–∑–∞–Ω–∞')}"
            )

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª –æ—á–∏—Å—Ç–∫–∏
        cleaning_rules = config.get("cleaning_rules", {})
        global_rules = cleaning_rules.get("global", [])
        specific_rules = cleaning_rules.get("document_specific", {})

        print(f"\nüßπ –ü–†–ê–í–ò–õ–ê –û–ß–ò–°–¢–ö–ò:")
        print(f"   üåê –ì–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª: {len(global_rules)}")
        print(f"   üéØ –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª: {len(specific_rules)}")

        for rule in global_rules[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
            print(f"     - {rule['name']}: {rule['description']}")

    def demo_performance_metrics(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        print("\n‚ö° –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–¢–†–ò–ö –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
        print("-" * 40)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
        history_file = self.base_dir / "processing_history.json"

        if not history_file.exists():
            print("‚ùå –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        with open(history_file, "r", encoding="utf-8") as f:
            history = json.load(f)

        if not history:
            print("‚ùå –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É—Å—Ç–∞")
            return

        print(f"üìä –ó–∞–ø–∏—Å–µ–π –≤ –∏—Å—Ç–æ—Ä–∏–∏: {len(history)}")

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        processing_times = [record["processing_time"] for record in history]
        quality_scores = [record["quality_after"] for record in history]

        avg_time = sum(processing_times) / len(processing_times)
        min_time = min(processing_times)
        max_time = max(processing_times)

        avg_quality = sum(quality_scores) / len(quality_scores)
        min_quality = min(quality_scores)
        max_quality = max(quality_scores)

        print(f"\n‚è±Ô∏è  –í–†–ï–ú–Ø –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {avg_time:.3f}—Å")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min_time:.3f}—Å")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max_time:.3f}—Å")

        print(f"\nüìä –ö–ê–ß–ï–°–¢–í–û –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {avg_quality:.1f}/100")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min_quality:.1f}/100")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max_quality:.1f}/100")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_types = {}
        for record in history:
            doc_type = record["document_type"]
            if doc_type not in doc_types:
                doc_types[doc_type] = {"count": 0, "avg_quality": 0, "avg_time": 0}

            doc_types[doc_type]["count"] += 1
            doc_types[doc_type]["avg_quality"] += record["quality_after"]
            doc_types[doc_type]["avg_time"] += record["processing_time"]

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        for doc_type, stats in doc_types.items():
            stats["avg_quality"] /= stats["count"]
            stats["avg_time"] /= stats["count"]

        print(f"\nüìã –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –î–û–ö–£–ú–ï–ù–¢–û–í:")
        for doc_type, stats in doc_types.items():
            print(f"   {doc_type}:")
            print(f"     –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {stats['count']}")
            print(f"     –°—Ä. –∫–∞—á–µ—Å—Ç–≤–æ: {stats['avg_quality']:.1f}/100")
            print(f"     –°—Ä. –≤—Ä–µ–º—è: {stats['avg_time']:.3f}—Å")

    def demo_comparison(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤"""

        print("\nüîÑ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–†–ê–í–ù–ï–ù–ò–Ø –ü–†–û–¶–ï–°–°–û–†–û–í")
        print("-" * 40)

        sample_file = self.conversion_dir / "–ê—Ä—Ö–∏–≤–∏—Å—Ç.md"

        if not sample_file.exists():
            print("‚ùå –§–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        print(f"üìÑ –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {sample_file.name}")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_file = self.base_dir / "temp_test.md"

        with open(sample_file, "r", encoding="utf-8") as src:
            with open(test_file, "w", encoding="utf-8") as dst:
                dst.write(src.read())

        processors = [
            ("–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", self.enhanced_processor),
            ("–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", self.intelligent_processor),
        ]

        results = {}

        for name, processor in processors:
            print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º: {name}")

            start_time = time.time()

            try:
                if name == "–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä":
                    result = processor.process_single_document(test_file)
                    processing_time = time.time() - start_time

                    results[name] = {
                        "success": result.success,
                        "quality_score": result.quality_score,
                        "document_type": result.document_type,
                        "processing_time": processing_time,
                        "issues": len(result.issues),
                        "improvements": len(result.improvements),
                    }
                else:
                    result = processor.process_document_intelligently(test_file)
                    processing_time = time.time() - start_time

                    results[name] = {
                        "success": result["success"],
                        "quality_score": (
                            result["quality_assessment"].overall_score
                            if result["success"]
                            else 0
                        ),
                        "document_type": (
                            result["document_type"] if result["success"] else "unknown"
                        ),
                        "processing_time": processing_time,
                        "issues": (
                            len(result["quality_assessment"].critical_issues)
                            if result["success"]
                            else 0
                        ),
                        "improvements": (
                            result["improvements_applied"] if result["success"] else 0
                        ),
                    }

                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ")
                print(f"   üìä –ö–∞—á–µ—Å—Ç–≤–æ: {results[name]['quality_score']:.1f}/100")
                print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {results[name]['processing_time']:.3f}—Å")

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                results[name] = {"success": False, "error": str(e)}

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        print(f"{'–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä':<25} {'–ö–∞—á–µ—Å—Ç–≤–æ':<10} {'–í—Ä–µ–º—è':<10} {'–£–ª—É—á—à–µ–Ω–∏—è':<12}")
        print("-" * 60)

        for name, result in results.items():
            if result["success"]:
                print(
                    f"{name:<25} {result['quality_score']:<10.1f} {result['processing_time']:<10.3f} {result['improvements']:<12}"
                )
            else:
                print(f"{name:<25} {'–û–®–ò–ë–ö–ê':<10} {'-':<10} {'-':<12}")

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if test_file.exists():
            test_file.unlink()

    def run_full_demo(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""

        print("üé≠ –ü–û–õ–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô")
        print("=" * 60)
        print("–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        demos = [
            ("NLP –ê–Ω–∞–ª–∏–∑", self.demo_nlp_analysis),
            ("–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞", self.demo_quality_assessment),
            ("–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", self.demo_content_analysis),
            ("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", self.demo_document_processing),
            ("–°–∏—Å—Ç–µ–º–∞ —à–∞–±–ª–æ–Ω–æ–≤", self.demo_template_system),
            ("–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", self.demo_performance_metrics),
            ("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤", self.demo_comparison),
        ]

        for demo_name, demo_func in demos:
            try:
                demo_func()
                time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è–º–∏
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ '{demo_name}': {e}")

        print(f"\nüéâ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 60)
        print("–í—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
        print("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Ä–µ–¥–µ.")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    demo = FeatureDemo()
    demo.run_full_demo()


if __name__ == "__main__":
    main()
