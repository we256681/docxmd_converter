#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
–í–∫–ª—é—á–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, NLP-–∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
"""

import hashlib
import json
import os
import re
import statistics
import time
from collections import Counter
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from .models import DocumentFeatures, ProcessingMetrics, QualityAssessment, ProcessingResult
from .nlp_analyzer import NLPAnalyzer
from .quality_assessor import IntelligentQualityAssessor


class IntelligentProcessor:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def __init__(self, config_path: str = None):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
        current_file = Path(__file__).resolve()
        self.base_dir = current_file.parent.parent.parent
        self.conversion_dir = self.base_dir / "docs" / "Conversion"

        if config_path is None:
            config_path = str(self.base_dir / "config" / "document_templates.json")

        self.config = self._load_config(config_path)
        self.nlp_analyzer = NLPAnalyzer()
        self.quality_assessor = IntelligentQualityAssessor()

        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.processing_history = []

    def _load_config(self, config_path: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {"templates": {}}

    def process_document_intelligently(self, file_path: str) -> Dict[str, Any]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        start_time = datetime.now()

        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # –û—Ç–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if "<!-- METADATA" in content:
                main_content, metadata_part = content.split("<!-- METADATA", 1)
            else:
                main_content = content
                metadata_part = ""

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            features = self.nlp_analyzer.extract_features(main_content)
            entities = self.nlp_analyzer.extract_entities(main_content)
            sentiment = self.nlp_analyzer.analyze_sentiment(main_content)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_type, confidence = self._intelligent_type_detection(
                main_content, features, entities or {}
            )

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            extracted_data = self._extract_structured_data(main_content, document_type)

            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            quality_assessment = self.quality_assessor.assess_quality(
                main_content, features, document_type, extracted_data or {}
            )

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
            improved_content = self._apply_intelligent_improvements(
                main_content, document_type, quality_assessment, extracted_data or {}
            )

            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            enhanced_metadata = self._create_enhanced_metadata(
                Path(file_path).name,
                document_type,
                confidence,
                features,
                entities or {},
                sentiment or {},
                quality_assessment,
                extracted_data or {},
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_content = improved_content.strip() + "\n\n" + enhanced_metadata

            with open(file_path, "w", encoding="utf-8") as f:
                f.write(final_content)

            processing_time = (datetime.now() - start_time).total_seconds()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            processing_record = {
                "filename": Path(file_path).name,
                "document_type": document_type,
                "confidence": confidence,
                "features": asdict(features),
                "quality_before": 0,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ü–µ–Ω–∫—É –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                "quality_after": quality_assessment.overall_score,
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat(),
            }

            self.processing_history.append(processing_record)

            return {
                "success": True,
                "document_type": document_type,
                "confidence": confidence,
                "features": features,
                "entities": entities or {},
                "sentiment": sentiment or {},
                "quality_assessment": quality_assessment,
                "processing_time": processing_time,
                "improvements_applied": len(quality_assessment.recommendations),
            }

        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
            }

    def _intelligent_type_detection(
        self, content: str, features: DocumentFeatures, entities: Dict
    ) -> Tuple[str, float]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        content_lower = content.lower()

        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        type_scores = {}

        # –î–æ–ª–∂–Ω–æ—Å—Ç–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
        job_desc_score = 0
        if "–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω" in content_lower and "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏" in content_lower:
            job_desc_score += 20
        if "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏" in content_lower:
            job_desc_score += 10
        if "–ø—Ä–∞–≤–∞" in content_lower:
            job_desc_score += 10
        if "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å" in content_lower:
            job_desc_score += 10
        if entities.get("positions"):
            job_desc_score += 15

        type_scores["–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–∞—è_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"] = job_desc_score

        # –û—Ç—á–µ—Ç
        report_score = 0
        if "–æ—Ç—á–µ—Ç" in content_lower:
            report_score += 20
        if "—Ä–µ–∑—É–ª—å—Ç–∞—Ç" in content_lower:
            report_score += 10
        if "–∞–Ω–∞–ª–∏–∑" in content_lower:
            report_score += 10
        if "–≤—ã–≤–æ–¥" in content_lower:
            report_score += 10
        if entities.get("dates"):
            report_score += 10

        type_scores["–æ—Ç—á–µ—Ç"] = report_score

        # –ü–æ–ª–æ–∂–µ–Ω–∏–µ
        regulation_score = 0
        if "–ø–æ–ª–æ–∂–µ–Ω–∏" in content_lower:
            regulation_score += 20
        if "–æ–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è" in content_lower:
            regulation_score += 15
        if "—Ü–µ–ª–∏" in content_lower and "–∑–∞–¥–∞—á–∏" in content_lower:
            regulation_score += 10

        type_scores["–ø–æ–ª–æ–∂–µ–Ω–∏–µ"] = regulation_score

        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
        instruction_score = 0
        if "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏" in content_lower and "–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω" not in content_lower:
            instruction_score += 20
        if "–ø–æ—Ä—è–¥–æ–∫" in content_lower:
            instruction_score += 10
        if "–∞–ª–≥–æ—Ä–∏—Ç–º" in content_lower:
            instruction_score += 10
        if features.list_count > 5:  # –ú–Ω–æ–≥–æ —Å–ø–∏—Å–∫–æ–≤ - –ø—Ä–∏–∑–Ω–∞–∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            instruction_score += 10

        type_scores["–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"] = instruction_score

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π —Ç–∏–ø
        if type_scores:
            best_type = max(type_scores.keys(), key=lambda k: type_scores[k])
            max_score = type_scores[best_type]

            if max_score > 0:
                confidence = min(1.0, max_score / 50.0)
                return best_type, confidence

        return "–æ–±—â–∏–π_–¥–æ–∫—É–º–µ–Ω—Ç", 0.1

    def _extract_structured_data(
        self, content: str, document_type: str
    ) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        extracted = {}
        content_lower = content.lower()

        # –û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        entities = self.nlp_analyzer.extract_entities(content)
        if entities:
            extracted.update(entities)

        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã–µ
        if document_type == "–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–∞—è_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è":
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_match = re.search(
                r"#\s*(?:–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–∞—è\s+–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è[:\s]*)?(.+)", content, re.IGNORECASE
            )
            if title_match:
                extracted["position"] = title_match.group(1).strip()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–∏
            functions_match = re.search(
                r"—Ñ—É–Ω–∫—Ü–∏–∏[:\s]*(.*?)(?=\n\n|\n##|$)", content_lower, re.DOTALL
            )
            if functions_match:
                extracted["functions"] = functions_match.group(1).strip()

        elif document_type == "–æ—Ç—á–µ—Ç":
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–∏–æ–¥ –æ—Ç—á–µ—Ç–∞
            period_patterns = [
                r"–∑–∞\s+(\d{4})\s+–≥–æ–¥",
                r"–∑–∞\s+(\w+\s+\d{4})",
                r"–ø–µ—Ä–∏–æ–¥[:\s]*(.+?)(?=\n|$)",
            ]

            for pattern in period_patterns:
                match = re.search(pattern, content_lower)
                if match:
                    extracted["report_period"] = match.group(1).strip()
                    break

        return extracted

    def _apply_intelligent_improvements(
        self,
        content: str,
        document_type: str,
        quality_assessment: QualityAssessment,
        extracted_data: Dict,
    ) -> str:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π"""

        improved = content

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        for issue in quality_assessment.critical_issues:
            if "–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã" in issue.lower():
                improved = self._remove_artifacts(improved)
            elif "–∑–∞–≥–æ–ª–æ–≤–∫–∏" in issue.lower():
                improved = self._add_missing_headers(improved, document_type)
            elif "–Ω–µ–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã" in issue.lower():
                improved = self._fill_empty_sections(
                    improved, document_type, extracted_data
                )

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        for recommendation in quality_assessment.recommendations:
            if "—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ" in recommendation.lower():
                improved = self._improve_formatting(improved)
            elif "—Å—Ç—Ä—É–∫—Ç—É—Ä—É" in recommendation.lower():
                improved = self._improve_structure(improved, document_type)
            elif "—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å" in recommendation.lower():
                improved = self._improve_consistency(improved)

        return improved

    def _remove_artifacts(self, content: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""

        # –£–¥–∞–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        content = re.sub(r"::: \{[^}]*\}", "", content)
        content = re.sub(r":::", "", content)
        content = re.sub(r"_{5,}", "", content)
        content = re.sub(r"\*{3,}", "", content)
        content = re.sub(r"_Toc\d+|_Ref\d+", "", content)

        # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        content = re.sub(r"\n{3,}", "\n\n", content)

        return content

    def _add_missing_headers(self, content: str, document_type: str) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""

        if document_type == "–¥–æ–ª–∂–Ω–æ—Å—Ç–Ω–∞—è_–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
            required_headers = [
                "## 1. –û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è",
                "## 2. –§—É–Ω–∫—Ü–∏–∏",
                "## 3. –î–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏",
                "## 4. –ü—Ä–∞–≤–∞",
                "## 5. –û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å",
            ]

            for header in required_headers:
                if header.lower() not in content.lower():
                    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –∫–æ–Ω–µ—Ü
                    content += f"\n\n{header}\n\n_{header[3:]} —Ç—Ä–µ–±—É–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è_"

        return content

    def _fill_empty_sections(
        self, content: str, document_type: str, extracted_data: Dict
    ) -> str:
        """–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤"""

        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–≥–ª—É—à–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if "position" in extracted_data:
            position = extracted_data["position"]
            content = content.replace(
                "_–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è_",
                f"{position} –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –∏ –ø–æ–¥—á–∏–Ω—è–µ—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—é.",
            )

        if "functions" in extracted_data:
            functions = extracted_data["functions"]
            content = content.replace(
                "_–§—É–Ω–∫—Ü–∏–∏ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è_", f"–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:\n{functions}"
            )

        return content

    def _improve_formatting(self, content: str) -> str:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        content = re.sub(r"^##([^\s#])", r"## \1", content, flags=re.MULTILINE)
        content = re.sub(r"^###([^\s#])", r"### \1", content, flags=re.MULTILINE)

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–ø–∏—Å–∫–∏
        content = re.sub(r"^(\s*)([‚Ä¢¬∑‚ñ™‚ñ´])\s*", r"\1- ", content, flags=re.MULTILINE)

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        content = re.sub(r" +$", "", content, flags=re.MULTILINE)
        content = re.sub(r" {2,}", " ", content)

        return content

    def _improve_structure(self, content: str, document_type: str) -> str:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ" not in content and content.count("##") > 3:
            headers = re.findall(r"^##\s+(.+)$", content, re.MULTILINE)
            if headers:
                toc = "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n\n"
                for i, header in enumerate(headers, 1):
                    toc += f"{i}. {header}\n"
                toc += "\n"

                # –í—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                content = re.sub(
                    r"(^#\s+.+\n\n)", r"\1" + toc, content, flags=re.MULTILINE
                )

        return content

    def _improve_consistency(self, content: str) -> str:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏"""

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
        lines = content.split("\n")
        in_numbered_section = False
        current_section = 0
        item_counter = 1

        for i, line in enumerate(lines):
            if re.match(r"^##\s+\d+\.", line):
                in_numbered_section = True
                current_section += 1
                item_counter = 1
            elif line.startswith("##"):
                in_numbered_section = False
            elif in_numbered_section and re.match(r"^\d+\.", line.strip()):
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
                lines[i] = re.sub(
                    r"^\d+\.", f"{current_section}.{item_counter}.", line.strip()
                )
                item_counter += 1

        return "\n".join(lines)

    def _create_enhanced_metadata(
        self,
        filename: str,
        document_type: str,
        confidence: float,
        features: DocumentFeatures,
        entities: Dict,
        sentiment: Dict,
        quality_assessment: QualityAssessment,
        extracted_data: Dict,
    ) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""

        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        metadata = {
            "created_at": now,
            "updated_at": now,
            "author": "intelligent_processor",
            "version": "6.0.0",
            "filename": filename,
            # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            "document_analysis": {
                "type": document_type,
                "type_confidence": round(confidence, 3),
                "features": asdict(features),
                "entities": entities,
                "sentiment": sentiment,
            },
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            "quality_assessment": {
                "overall_score": round(quality_assessment.overall_score, 1),
                "structure_score": round(quality_assessment.structure_score, 1),
                "content_score": round(quality_assessment.content_score, 1),
                "formatting_score": round(quality_assessment.formatting_score, 1),
                "completeness_score": round(quality_assessment.completeness_score, 1),
                "consistency_score": round(quality_assessment.consistency_score, 1),
                "recommendations_count": len(quality_assessment.recommendations),
                "critical_issues_count": len(quality_assessment.critical_issues),
            },
            # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            "extracted_data": extracted_data,
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            "processing_metadata": {
                "method": "intelligent_nlp_processing",
                "ai_enhanced": True,
                "quality_assured": True,
                "machine_learning_ready": True,
            },
        }

        return (
            f"<!-- METADATA\n{json.dumps(metadata, indent=2, ensure_ascii=False)}\n-->"
        )

    def process_all_documents(self) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""

        print("üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("=" * 60)

        md_files = list(self.conversion_dir.glob("**/*.md"))

        if not md_files:
            print("‚ùå –§–∞–π–ª—ã MD –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return {}

        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(md_files)}")

        results = []
        total_processing_time = 0

        for md_file in md_files:
            print(f"\nüîÑ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {md_file.name}")

            result = self.process_document_intelligently(str(md_file))
            results.append(result)

            if result["success"]:
                print(
                    f"   üß† –¢–∏–ø: {result['document_type']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f})"
                )
                print(
                    f"   üìä –ö–∞—á–µ—Å—Ç–≤–æ: {result['quality_assessment'].overall_score:.1f}/100"
                )
                print(f"   üîß –£–ª—É—á—à–µ–Ω–∏–π: {result['improvements_applied']}")
                print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {result['processing_time']:.2f}—Å")
                total_processing_time += result["processing_time"]
            else:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {result['error']}")

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        successful_results = [r for r in results if r["success"]]
        avg_quality = 0
        avg_confidence = 0

        if successful_results:
            avg_quality = statistics.mean(
                r["quality_assessment"].overall_score for r in successful_results
            )
            avg_confidence = statistics.mean(
                r["confidence"] for r in successful_results
            )

            print(f"\nüìà –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print("=" * 60)
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(successful_results)}")
            print(f"üìä –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {avg_quality:.1f}/100")
            print(f"üéØ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–∏–ø–µ: {avg_confidence:.2f}")
            print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_processing_time:.2f}—Å")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            type_counts = Counter(r["document_type"] for r in successful_results)
            print(f"\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:")
            for doc_type, count in type_counts.items():
                print(f"   - {doc_type}: {count}")

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_distribution = {
                "excellent": len(
                    [
                        r
                        for r in successful_results
                        if r["quality_assessment"].overall_score >= 90
                    ]
                ),
                "high": len(
                    [
                        r
                        for r in successful_results
                        if 80 <= r["quality_assessment"].overall_score < 90
                    ]
                ),
                "medium": len(
                    [
                        r
                        for r in successful_results
                        if 70 <= r["quality_assessment"].overall_score < 80
                    ]
                ),
                "low": len(
                    [
                        r
                        for r in successful_results
                        if r["quality_assessment"].overall_score < 70
                    ]
                ),
            }

            print(f"\nüèÜ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞:")
            print(f"   ü•á –û—Ç–ª–∏—á–Ω–æ–µ (90+): {quality_distribution['excellent']}")
            print(f"   ü•à –í—ã—Å–æ–∫–æ–µ (80-89): {quality_distribution['high']}")
            print(f"   ü•â –°—Ä–µ–¥–Ω–µ–µ (70-79): {quality_distribution['medium']}")
            print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ (<70): {quality_distribution['low']}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self._save_processing_history()

        return {
            "total_files": len(md_files),
            "successful": len(successful_results),
            "failed": len(results) - len(successful_results),
            "avg_quality": avg_quality,
            "avg_confidence": avg_confidence,
            "total_time": total_processing_time,
            "results": results,
        }

    def _save_processing_history(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

        history_file = self.base_dir / "data" / "processing_history.json"

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é
            if history_file.exists():
                with open(history_file, "r", encoding="utf-8") as f:
                    existing_history = json.load(f)
            else:
                existing_history = []

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
            existing_history.extend(self.processing_history)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
            with open(history_file, "w", encoding="utf-8") as f:
                json.dump(existing_history, f, indent=2, ensure_ascii=False)

            print(
                f"\nüíæ –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {len(self.processing_history)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π"
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")

    def get_processing_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

        if not self.processing_history:
            return {"message": "–ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É—Å—Ç–∞"}

        total_files = len(self.processing_history)
        avg_processing_time = sum(r['processing_time'] for r in self.processing_history) / total_files
        avg_quality = sum(r['quality_after'] for r in self.processing_history) / total_files

        document_types = {}
        for record in self.processing_history:
            doc_type = record['document_type']
            if doc_type not in document_types:
                document_types[doc_type] = 0
            document_types[doc_type] += 1

        return {
            'total_files_processed': total_files,
            'average_processing_time': avg_processing_time,
            'average_quality_score': avg_quality,
            'document_types_distribution': document_types,
            'last_processed': self.processing_history[-1]['timestamp'] if self.processing_history else None
        }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    processor = IntelligentProcessor()
    results = processor.process_all_documents()

    print(f"\nüéâ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {processor.conversion_dir}")


if __name__ == "__main__":
    main()
